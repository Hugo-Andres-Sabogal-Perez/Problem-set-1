DF = DF[DF$relab != 8,]
# Se estandarizan los datos ordinales y contunuos:
DF = DF %>% mutate_at(c(continuas, ordinales), ~(scale(.) %>% as.vector))
# Se crean dummys para las categoricas:
DF = get_dummies(
DF,
cols = categoricas,
prefix = TRUE,
prefix_sep = "_",
drop_first = FALSE,
dummify_na = TRUE)
DF = DF %>% select(-categoricas)
ED = ED[!(ED$Variable %in% categoricas),]
# Se limpian las binarias:
resta1 = function(x){
y = x-1
returnValue(y)
}
DF = DF %>% mutate_at(binar12, ~ (resta1(.)))
# Se escogen variables que no tengan covarianza 1 con otras:
Corr = as.data.frame(cor(DF, use = 'complete.obs'))
ED$Corr = as.vector(rep(NA, nrow(ED)))
for(var in rownames(Corr)){
COR = Corr %>% select(var)
names = colnames(Corr)[abs(COR) > 0.999]
names = names[!is.na(names)]
ED$Corr[ED$Variable == var] = toString.default(names)
}
# Variables a descartar:
r = c('y_total_m', 'y_total_m_ha', 'y_ingLab_m', 'p6500', 'informal', 'p6100', 'fex_c', 'fex_dpto',
'fweight', 'iof6', 'iof2', 'iof1', 'p7070')
DF = DF %>% select(-r)
ED = ED[!(ED$Variable %in% r),]
# Imputación de variables por knn:
# k = 5
DFimp = kNN(DF)
DFimp = DFimp[,1:89]
# Test y train:
set.seed(10101)
inTrain <- createDataPartition(
y = DFimp$lnw,  ## the outcome data are needed
p = .70, ## The percentage of data in the
list = FALSE
)
training <- DFimp[ inTrain,]
testing  <- DFimp[-inTrain,]
# Formula general para la selección de variables:
varspoly = ED$Variable[ED$Tipo == 'continua']
varsum = ED$Variable[ED$Tipo != 'continua']
varcat = colnames(DFimp)[57:89]
maxmodel = as.formula(paste("lnw ~", paste(varsum, collapse = '+'), '+',paste(varcat, collapse = '+'),'+',paste("poly(", varspoly , ", 2, raw=TRUE)", collapse = " + ")))
# Modelo Forward Selection:
set.seed(10101)
folds = sample(rep (1:10, length = nrow(DFimp)))
crossval = matrix (NA, 10, 50, dimnames = list (NULL , paste (1:50)))
for (j in 1:10) {
fit = regsubsets(maxmodel, data = DFimp[folds != j,], nvmax = 50, method = "forward")
test = model.matrix(maxmodel, data = DFimp[folds == j,])
for (i in 1:50) {
coefi = coef(fit, id = i)
pred = test[,names(coefi)]%*%coefi
crossval[j, i] = mean((DFimp$lnw[folds == j] - pred)^2)
}
}
# Se calcula la raiz del error cuadratico medio:
errforward = apply (crossval , 2, mean)
nvars = which.min(errforward)[[1]]
#Se estima el mejor modelo con 24 variables elegidas mediante el algoritmo forward.
forward_model = regsubsets(maxmodel,
data = DFimp,
nvmax = nvars,
method = "forward")
forward_model_names = names(coef(forward_model, id=nvars))
# Se extrae el RMSE del modelo:
score5 = errforward[nvars]
# Se plentea la forma funcional de forward selection:
formfor = as.formula(lnw ~ sex + p6210 +
oficio + college + totalHoursWorked +
formal + sizeFirm + p6240_3 +
p6240_4 + p6585s1a1 + p6585s2a1 +
p6585s4a1 + p6610s1 + p6630s6a1 +
p6630s6a1^2 + p7500s3a1^2 + iof3i^2 +
ingtotob^2 + ingtot + p6920_3 +
iof3h + ingtotob)
# Se calcula el RMSE fuera de muestra:
modelo5 = lm(formfor, data = training )
predictions = predict(modelo5, testing)
score5a = RMSE(predictions, testing$lnw)
# Modelo Backward Selection:
for (j in 1:10) {
fit = regsubsets(maxmodel, data = DFimp[folds != j,], nvmax = 50, method = "backward")
test = model.matrix(maxmodel, data = DFimp[folds == j,])
for (i in 1:50) {
coefi = coef(fit, id = i)
pred = test[,names(coefi)]%*%coefi
crossval[j, i] = mean((DFimp$lnw[folds == j] - pred)^2)
}
}
# Se calcula la raiz del error cuadratico medio:
errbackward = apply (crossval , 2, mean)
nvars = which.min(errbackward)[[1]]
#Se estiam el mejor modelo de 35 variables:
backward_model = regsubsets(maxmodel,
data = DFimp,
nvmax = nvars,
method = "backward")
backward_model_names = names(coef(backward_model, id = nvars))
# Se extrae el RMSE del modelo de 35 variables:
score6 = errbackward[nvars]
# Se plantea la forma funcional del modelo:
formback = as.formula(lnw ~ sex + p6210 +
p6210s1 + oficio + p7505 +
college + totalHoursWorked + sizeFirm +
p6240_4 + p6920_2 +p6426^2 +
p6585s1a1 + p6585s1a1^2 + p6585s2a1 +
p6585s3a1 + p6585s4a1 + p6585s4a1^2 +
p6600s1^2 + p6610s1 + p6620s1 +
p6630s6a1 + p6630s6a1^2 + p7500s1a1 +
p7500s1a1^2 +p7500s2a1^2 + p7500s3a1^2 +
p7510s3a1 + ingtotob^2 + ingtot +
ingtot^2 + mes_12 + p6920_3 + iof3h +
ingtotob)
# Se calcula el RMSE fuera de muestra:
modelo6 = lm(formback, data = training )
predictions = predict(modelo6, testing)
score6a = RMSE(predictions, testing$lnw)
# Interseccion entre modelos:
# Creo la matriz de predictores:
formint = as.formula(lnw ~ sex + p6210 + oficio +
college + totalHoursWorked + sizeFirm +
p6240_4 + p6585s1a1 + p6585s2a1 +
p6585s4a1 + p6610s1 +  p6630s6a1 + p6630s6a1^2 +
p7500s3a1^2 + ingtotob^2 + ingtot +
p6920_3 + iof3h + ingtotob)
# Se calcula el RMSE fuera de muestra:
modelo7 = lm(formint, data = training)
predictions = predict(modelo7, testing)
score7 = RMSE(predictions, testing$lnw)
# Se crea un data frame con los errores de cada modelo:
ERR = data.frame('MODELO' = rep(NA, 7), 'MSE(Muestral)' =  rep(NA, 7))
ERR[1,] = c('Modelo 1', score1a)
ERR[2,] = c('Modelo 2', score2a)
ERR[3,] = c('Modelo 3', score3a)
ERR[4,] = c('Modelo 4', score4a)
ERR[5,] = c('Modelo 5', score5a)
ERR[6,] = c('Modelo 6', score6a)
ERR[7,] = c('Modelo 7', score7)
stargazer(ERR, summary = F, type = 'text', out='Views/tabla_mse.tex')
stargazer(ERR, summary = F, type = 'latex', out='Views/tabla_mse.tex')
install.packages("styler")
styler:::style_selection()
styler:::style_selection()
styler:::style_selection()
styler:::style_selection()
styler:::set_style_transformers()
styler:::style_active_file()
DF <- import("Stores/DF.csv")
# Realizamos inicialmente una limpieza del entorno
rm(list = ls())
# Llamamos las librerías necesarias para la realización del trabajo
require(pacman)
require(tidyverse)
require(rvest)
require(stargazer)
require(rio)
require(caret)
require(gridExtra)
require(skimr)
require(boot)
require(tidytable)
require(VIM)
require(leaps)
require(margins)
DF <- import("Stores/DF.csv")
DF <- import("Stores/DF.csv")
# Se crea una base para guardar las estadisticas descriptivas más relevantes para el trabajo:
vars <- length(colnames(DF))
ED <- data.frame("Variable" = colnames(DF), "Missings" = rep(NA, vars), "Media" = rep(NA, vars), "Desviacion Estandard" = rep(NA, vars))
# Se cuentan los missings y se calcula la media y la desviación estándar de la muestra:
for (col in colnames(DF)) {
df <- DF[, colnames(DF) == col]
NAs <- sum(is.na(df))
mean <- mean(df, na.rm = T)
sd <- sqrt(var(df, na.rm = T))
ED[ED$Variable == col, 2] <- NAs
ED[ED$Variable == col, 3] <- mean
ED[ED$Variable == col, 4] <- sd
}
# 1. Limpieza de datos:
# Se eliminan las constantes (u observaciones que tienen desviación estándar igual a cero) y las variables sin observaciones (missings):
C <- ED %>%
filter(Desviacion.Estandard == 0 | is.na(Desviacion.Estandard)) %>%
select(Variable) %>%
as.vector()
ED <- ED %>% filter(Desviacion.Estandard != 0 | !is.na(Desviacion.Estandard))
ED <- ED %>% filter(Variable != "cuentaPropia")
DF <- DF[!is.na(DF$y_ingLab_m_ha), ]
DF <- DF %>% select(-C$Variable)
DF <- DF[DF$age != 78, ]
DF <- DF[!(rownames(DF) %in% c("5733", "579")), ]
# 2. Estadisticas descriptivas:
base <- DF %>% select(age, oficio, formal, maxEducLevel, orden, p7040, sex, sizeFirm, y_ingLab_m_ha, hoursWorkUsual)
base$p7040[base$p7040 == 2] <- 0
base$ln_sal <- log(base$y_ingLab_m_ha) # Se crea el logaritmo del salario por horas para normalizar los valores de la variable.
stargazer(base, type = "text", summary = T, title = "Estadisticas Descriptivas", out = "Views/esta_des.txt")
base$age_2 <- base$age^2
modelo1 <- lm(ln_sal ~ age + I(age^2), data = base)
mar <- summary(margins(modelo1))
base$Female <- ifelse(base$sex == 0, 1, 0) # cambiamos la variable sexo dado que ésta inicialmente toma el valor de 1 si la persona es hombre y 0 d.l.c para que tome el valor de 1 si la persona es mujer y 0 d.l.c y así correr el modelo con la que realmente se requiere en las instrucciones
modelo2 <- lm(ln_sal ~ Female, data = base)
# 5.Predicting Earnings
set.seed(10101)
inTrain <- createDataPartition(
y = base$ln_sal, ## the outcome data are needed
p = .70, ## The percentage of data in the
list = FALSE
)
training <- base[inTrain, ]
testing <- base[-inTrain, ]
# Training 7 models
# 1.
form_1 <- ln_sal ~ age + age_2
modelo1a <- lm(form_1,
data = training
)
predictions1 <- predict(modelo1a, testing)
score1a <- RMSE(predictions1, testing$ln_sal)
score1a
# 2.
testing_of <- testing
testing_of$oficio <- factor(testing_of$oficio, exclude = c(73, 63))
form_2 <- ln_sal ~ Female + age + factor(maxEducLevel) + formal + factor(oficio) + hoursWorkUsual + p7040 + factor(sizeFirm)
modelo2a <- lm(form_2,
data = training
)
predictions2 <- predict(modelo2a, testing_of)
data_mod2a <- data.frame("ln_sal" = testing_of$ln_sal, "pred2" = predictions2)
data_mod2a <- na.omit(data_mod2a)
score2a <- RMSE(pred = data_mod2a$pred2, obs = data_mod2a$ln_sal)
score2a
# 3.
form_3 <- ln_sal~ age + age^2 + age^3 + age^4 + age^5 + age^6 + age^7 + age^8
modelo3a <- lm(form_3, data = training)
predictions <- predict(modelo3a, testing)
score3a <- RMSE(predictions, testing$ln_sal)
score3a
# 4.
form_4 <- ln_sal ~ age + age_2 +
poly(age, 3, raw = TRUE):Female +
poly(age, 3, raw = TRUE):factor(maxEducLevel) +
poly(age, 3, raw = TRUE):formal +
poly(age, 3, raw = TRUE):factor(oficio) +
poly(age, 3, raw = TRUE):hoursWorkUsual +
poly(age, 3, raw = TRUE):p7040 +
poly(age, 3, raw = TRUE):sizeFirm
modelo4a <- lm(form_4,
data = training
)
predictions4 <- predict(modelo4a, testing_of)
data_mod4a <- data.frame("ln_sal" = testing_of$ln_sal, "pred4" = predictions4)
data_mod4a <- na.omit(data_mod4a)
score4a <- RMSE(pred = data_mod4a$pred4, obs = data_mod4a$ln_sal)
score4a
# 5.
# Se admitirán 20% de datos faltantes como máximo:
porcentaje_obs <- nrow(DF) * 0.2
Miss <- ED[ED$Missings < porcentaje_obs, 1]
DF <- DF %>% select(Miss$Variable)
ED <- ED[ED$Missings < porcentaje_obs, ]
# Se crea logaritmo del salario:
DF$lnw <- log(DF$y_ingLab_m_ha)
# Clasificación por tipo de variable:
ED$Tipo <- rep(NA, nrow(ED))
ordinales <- c(
"estrato1", "age", "p6100", "p6210", "p6210s1", "oficio", "relab", "p6870", "fex_dpto", "hoursWorkUsual",
"fweight", "maxEducLevel", "regSalud", "totalHoursWorked", "sizeFirm"
)
categoricas <- c("mes", "p6050", "p6240", "p6920", "cotPension")
binarias <- c("sex", "college", "formal", "informal", "microEmpresa")
binar12 <- c("p7040", "p7090", "p7495", "p7505", "p6090")
continuas <- c(
"p6426", "p6500", "p6510s1", "p6545s1", "p6580s1", "p6585s1a1", "p6585s2a1", "p6585s3a1", "p6585s4a1",
"p6590s1", "p6600s1", "p6610s1", "p6620s1", "p6630s1a1", "p6630s2a1", "p6630s3a1", "p6630s4a1", "p6630s6a1",
"p7070", "p7500s1a1", "p7500s2a1", "p7500s3a1", "p7510s1a1", "p7510s2a1", "p7510s3a1", "p7510s5a1", "p7510s6a1",
"p7510s7a1", "impa", "isa", "ie", "iof1", "iof2", "iof3h", "iof3i", "iof6", "ingtotob", "ingtot", "fex_c",
"y_salary_m", "y_salary_m_hu", "y_ingLab_m", "y_total_m", "y_total_m_ha"
)
rem <- c(
"p6510", "p6545", "p6580", "p6585s1", "p6585s2", "p6585s3", "p6585s4", "p6590", "p6600", "p6610", "p6620", "p6630s1",
"p6630s2", "p6630s3", "p6630s4", "p6630s6", "directorio", "secuencia_p", "orden", "y_ingLab_m_ha"
)
DF <- DF %>% select(-rem)
ED <- ED[!(ED$Variable %in% rem), ]
ED <- ED %>% mutate(Tipo = case_when(
Variable %in% continuas ~ "continua",
Variable %in% ordinales ~ "ordi",
Variable %in% categoricas ~ "cat",
Variable %in% c(binarias, binar12) ~ "dummy"
))
# Eliminacion de outliers de categoricas:
DF <- DF[DF$p6090 != 9, ]
DF <- DF[DF$p6100 != 9, ]
DF <- DF[DF$p6210 != 9, ]
DF <- DF[DF$relab != 8, ]
# Se estandarizan los datos ordinales y contunuos:
DF <- DF %>% mutate_at(c(continuas, ordinales), ~ (scale(.) %>% as.vector()))
# Se crean dummys para las categoricas:
DF <- get_dummies(
DF,
cols = categoricas,
prefix = TRUE,
prefix_sep = "_",
drop_first = FALSE,
dummify_na = TRUE
)
DF <- DF %>% select(-categoricas)
ED <- ED[!(ED$Variable %in% categoricas), ]
# Se limpian las binarias:
resta1 <- function(x) {
y <- x - 1
returnValue(y)
}
DF <- DF %>% mutate_at(binar12, ~ (resta1(.)))
# Se escogen variables que no tengan covarianza 1 con otras:
Corr <- as.data.frame(cor(DF, use = "complete.obs"))
ED$Corr <- as.vector(rep(NA, nrow(ED)))
for (var in rownames(Corr)) {
COR <- Corr %>% select(var)
names <- colnames(Corr)[abs(COR) > 0.999]
names <- names[!is.na(names)]
ED$Corr[ED$Variable == var] <- toString.default(names)
}
# Variables a descartar:
r <- c(
"y_total_m", "y_total_m_ha", "y_ingLab_m", "p6500", "informal", "p6100", "fex_c", "fex_dpto",
"fweight", "iof6", "iof2", "iof1", "p7070"
)
DF <- DF %>% select(-r)
ED <- ED[!(ED$Variable %in% r), ]
# Imputación de variables por knn:
# k = 5
DFimp <- kNN(DF)
DFimp <- DFimp[, 1:89]
# Test y train:
set.seed(10101)
inTrain <- createDataPartition(
y = DFimp$lnw, ## the outcome data are needed
p = .70, ## The percentage of data in the
list = FALSE
)
training <- DFimp[inTrain, ]
testing <- DFimp[-inTrain, ]
# Formula general para la selección de variables:
varspoly <- ED$Variable[ED$Tipo == "continua"]
varsum <- ED$Variable[ED$Tipo != "continua"]
varcat <- colnames(DFimp)[57:89]
maxmodel <- as.formula(paste("lnw ~", paste(varsum, collapse = "+"), "+", paste(varcat, collapse = "+"), "+", paste("poly(", varspoly, ", 2, raw=TRUE)", collapse = " + ")))
# Modelo Forward Selection:
set.seed(10101)
folds <- sample(rep(1:10, length = nrow(DFimp)))
crossval <- matrix(NA, 10, 50, dimnames = list(NULL, paste(1:50)))
for (j in 1:10) {
fit <- regsubsets(maxmodel, data = DFimp[folds != j, ], nvmax = 50, method = "forward")
test <- model.matrix(maxmodel, data = DFimp[folds == j, ])
for (i in 1:50) {
coefi <- coef(fit, id = i)
pred <- test[, names(coefi)] %*% coefi
crossval[j, i] <- mean((DFimp$lnw[folds == j] - pred)^2)
}
}
# Se calcula la raiz del error cuadratico medio:
errforward <- apply(crossval, 2, mean)
nvars <- which.min(errforward)[[1]]
# Se estima el mejor modelo con 24 variables elegidas mediante el algoritmo forward.
forward_model <- regsubsets(maxmodel,
data = DFimp,
nvmax = nvars,
method = "forward"
)
forward_model_names <- names(coef(forward_model, id = nvars))
# Se extrae el RMSE del modelo:
score5 <- errforward[nvars]
# Se plentea la forma funcional de forward selection:
formfor <- as.formula(lnw ~ sex + p6210 +
oficio + college + totalHoursWorked +
formal + sizeFirm + p6240_3 +
p6240_4 + p6585s1a1 + p6585s2a1 +
p6585s4a1 + p6610s1 + p6630s6a1 +
p6630s6a1^2 + p7500s3a1^2 + iof3i^2 +
ingtotob^2 + ingtot + p6920_3 +
iof3h + ingtotob)
# Se calcula el RMSE fuera de muestra:
modelo5 <- lm(formfor, data = training)
predictions <- predict(modelo5, testing)
score5a <- RMSE(predictions, testing$lnw)
# Modelo Backward Selection:
for (j in 1:10) {
fit <- regsubsets(maxmodel, data = DFimp[folds != j, ], nvmax = 50, method = "backward")
test <- model.matrix(maxmodel, data = DFimp[folds == j, ])
for (i in 1:50) {
coefi <- coef(fit, id = i)
pred <- test[, names(coefi)] %*% coefi
crossval[j, i] <- mean((DFimp$lnw[folds == j] - pred)^2)
}
}
# Se calcula la raiz del error cuadratico medio:
errbackward <- apply(crossval, 2, mean)
nvars <- which.min(errbackward)[[1]]
# Se estiam el mejor modelo de 35 variables:
backward_model <- regsubsets(maxmodel,
data = DFimp,
nvmax = nvars,
method = "backward"
)
backward_model_names <- names(coef(backward_model, id = nvars))
# Se extrae el RMSE del modelo de 35 variables:
score6 <- errbackward[nvars]
# Se plantea la forma funcional del modelo:
formback <- as.formula(lnw ~ sex + p6210 +
p6210s1 + oficio + p7505 +
college + totalHoursWorked + sizeFirm +
p6240_4 + p6920_2 + p6426^2 +
p6585s1a1 + p6585s1a1^2 + p6585s2a1 +
p6585s3a1 + p6585s4a1 + p6585s4a1^2 +
p6600s1^2 + p6610s1 + p6620s1 +
p6630s6a1 + p6630s6a1^2 + p7500s1a1 +
p7500s1a1^2 + p7500s2a1^2 + p7500s3a1^2 +
p7510s3a1 + ingtotob^2 + ingtot +
ingtot^2 + mes_12 + p6920_3 + iof3h +
ingtotob)
# Se calcula el RMSE fuera de muestra:
modelo6 <- lm(formback, data = training)
predictions <- predict(modelo6, testing)
score6a <- RMSE(predictions, testing$lnw)
# Interseccion entre modelos:
# Creo la matriz de predictores:
formint <- as.formula(lnw ~ sex + p6210 + oficio +
college + totalHoursWorked + sizeFirm +
p6240_4 + p6585s1a1 + p6585s2a1 +
p6585s4a1 + p6610s1 + p6630s6a1 + p6630s6a1^2 +
p7500s3a1^2 + ingtotob^2 + ingtot +
p6920_3 + iof3h + ingtotob)
# Se calcula el RMSE fuera de muestra:
modelo7 <- lm(formint, data = training)
predictions <- predict(modelo7, testing)
score7 <- RMSE(predictions, testing$lnw)
# Se crea un data frame con los errores de cada modelo:
ERR <- data.frame("MODELO" = rep(NA, 7), "MSE(Muestral)" = rep(NA, 7))
ERR[1, ] <- c("Modelo 1", score1a)
ERR[2, ] <- c("Modelo 2", score2a)
ERR[3, ] <- c("Modelo 3", score3a)
ERR[4, ] <- c("Modelo 4", score4a)
ERR[5, ] <- c("Modelo 5", score5a)
ERR[6, ] <- c("Modelo 6", score6a)
ERR[7, ] <- c("Modelo 7", score7)
stargazer(ERR, summary = F, type = "latex", out = "Views/tabla_mse.tex")
# Errores de prediccion en el test (modelo 6):
predictions = predict(modelo6, testing)
MSEdist = data.frame('ID' = rownames(testing), 'salario_real' = testing$lnw, 'salario_predicho' = predictions)
MSEdist$MSE = (MSEdist$salario_real - MSEdist$salario_predicho)^2
hist(MSEdist$MSE)
hist(MSEdist$MSE)
predictions = predict(modelo6, testing)
MSEdist = data.frame('ID' = rownames(testing), 'salario_real' = testing$lnw, 'salario_predicho' = predictions)
MSEdist$MSE = (MSEdist$salario_real - MSEdist$salario_predicho)^2
hist(MSEdist$MSE)
sd = sqrt(var(MSEdist$MSE))
crit = 3*sd
cirt
crit
histograma_mse <- ggplot(MSEdist, aes(x = MSE)) +
geom_histogram(color = "white", fill = "darkblue") +
xlab("MSE del modelo 6") +
ylab("Frecuencia") +
theme_bw()
histograma_MSE
histograma_mse
histograma_mse <- ggplot(MSEdist, aes(x = MSE)) +
geom_histogram(color = "white", fill = "darkblue", bins=30) +
xlab("MSE del modelo 6") +
ylab("Frecuencia") +
theme_bw()
histograma_mse
histograma_mse <- ggplot(MSEdist, aes(x = MSE)) +
geom_histogram(color = "white", fill = "darkblue", bins=40) +
xlab("MSE del modelo 6") +
ylab("Frecuencia") +
theme_bw()
histograma_mse
histograma_mse <- ggplot(MSEdist, aes(x = MSE)) +
geom_histogram(color = "darkblue",fill = "darkblue", bins=40) +
xlab("MSE del modelo 6") +
ylab("Frecuencia") +
theme_bw()
histograma_mse
sqrt(1.72)
ggsave("Views/histograma_mse.pdf", width = 6, height = 4, plot = histograma_mse)
getwd()
setwd("C:\Users\hugos\OneDrive - Universidad de los andes\contenido semestres\9 semestre\Big data y machine learning\talleres\taller 1\github\scripts")
setwd("C:/Users/hugos/OneDrive - Universidad de los andes/contenido semestres/9 semestre/Big data y machine learning/talleres/taller 1/github/scripts")
directorio<-'C:/Users/hugos/OneDrive - Universidad de los andes/contenido semestres/9 semestre/Big data y machine learning/talleres/taller 1/github/scripts'
nuevo_dic <- substr(directorio, 1, nchar(directorio) - 7)
nuevo_dic
nuevo_dic <- substr(directorio, 1, nchar(directorio) - 8)
nuevo_dic
setwd("C:/Users/hugos/OneDrive - Universidad de los andes/contenido semestres/9 semestre/Big data y machine learning/talleres/taller 1/github/scripts")
print(substr(getwd(), 1, nchar(getwd()) - 8))
#### Script problem set 1 ######
# Realizamos inicialmente una limpieza del entorno
rm(list = ls())
setwd(substr(getwd(), 1, nchar(getwd()) - 8))
getwd()
